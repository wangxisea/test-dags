apiVersion: "sparkoperator.k8s.io/v1beta1"
kind: SparkApplication
metadata:
  name: trx-dedup
  namespace: airflow
  app: trx-dedup
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: 631741882205.dkr.ecr.eu-central-1.amazonaws.com/infin-data-mlflow:trxdedup-0.1
  imagePullPolicy: Always
  mainApplicationFile: local:///app/etl/dedup_transactions.py
  arguments: ["--runtime_config", "/app/etl/config/runtime.config"]
  sparkVersion: "2.4.0"
  sparkConf:
    "spark.executor.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4=true"
    "spark.driver.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4=true"
  hadoopConf:
    "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "com.amazonaws.services.s3.enableV4": "true"
    "fs.s3a.endpoint": "s3.eu-central-1.amazonaws.com"
  restartPolicy:
    type: Never
    # type: OnFailure
    # onFailureRetries: 1
    # onFailureRetryInterval: 10
    # onSubmissionFailureRetries: 5
    # onSubmissionFailureRetryInterval: 20
  driver:
    cores: 0.1
    coreLimit: "500m"
    memory: "1g"
    labels:
      version: 2.4.0
    serviceAccount: default
  executor:
    cores: 1
    instances: 1
    memory: "1g"
    labels:
      version: 2.4.0
  # monitoring:
  #   monitoring:
  #   exposeDriverMetrics: true
  #   exposeExecutorMetrics: true
  #   prometheus:
  #     jmxExporterJar: "/opt/spark/jars/jmx_prometheus_javaagent-0.3.1.jar"
  #     port: 8090